import {
  GESPREK_TARGET_MESSAGES_COUNT,
  GESPREK_TURNS_PER_BATCH,
} from "@/config";
import { Deelnemer } from "@/data/deelnemers";
import { getRandomModelConfig } from "@/lib/ai";
import { generateObject } from "ai";
import { z } from "zod";
import { addTimestampsToMessages } from "../utils";
import { createLogger } from "@/lib/logger";

const logger = createLogger("genereer-gesprek-berichten");

// Constants for conversation context management
const CONTEXT_THRESHOLD = 15; // Show summary when conversation has >15 messages
const RECENT_MESSAGES_COUNT = 5; // Number of recent messages to show for reactions
const LEAST_ACTIVE_THRESHOLD = 3; // Minimum message count to be considered "active"
const MAX_TURNS_TO_USE = 3; // Maximum turns to use from AI response (even if it generates more)
const LAST_BATCH_THRESHOLD = 10; // Remaining messages threshold to trigger last batch mode

export type GeneratedMessage = {
  message: string;
  deelnemerName: string;
  deelnemerId: number;
};

type AIGeneratedMessage = {
  deelnemerName: string;
  messages: string[]; // 1-3 berichten per burst
};

/**
 * Build a Zod schema for the messages that can be generated by the AI, linked to enumerate of available deelnemers.
 * @param deelnemers - The deelnemers of the conversation.
 * @returns A Zod schema for the messages of the conversation.
 */
export function buildMessageSchema(deelnemers: Deelnemer[]) {
  const deelnemerNames = deelnemers.map((d) => d.name);

  const MIN_TURNS = 2;
  const MAX_TURNS = 10; // Ruime limiet voor schema, we truncaten naar 3 in code
  const MIN_MESSAGES_PER_TURN = 1;
  const MAX_MESSAGES_PER_TURN = 3;

  return z
    .array(
      z
        .object({
          deelnemerName: z
            .enum(deelnemerNames as [string, ...string[]])
            .describe(
              `De naam van de politicus die deze berichten verstuurt. Moet een van de volgende zijn: ${deelnemerNames.join(
                ", "
              )}`
            ),
          messages: z
            .array(z.string())
            .min(MIN_MESSAGES_PER_TURN)
            .max(MAX_MESSAGES_PER_TURN)
            .describe(
              `Array van ${MIN_MESSAGES_PER_TURN} tot ${MAX_MESSAGES_PER_TURN} korte chat-berichten van deze politicus. MEESTAL 1 bericht, SOMS 2 berichten, ZELDEN 3 berichten. Minder is meer! Elk bericht is zeer kort (1 zin of minder).`
            ),
        })
        .describe("Een enkele beurt: één politicus die 1-3 berichten stuurt")
    )
    .min(MIN_TURNS)
    .max(MAX_TURNS)
    .describe(
      `Array van gespreksbeurten. Bij voorkeur 2-3 beurten. Elke beurt = één politicus die reageert met 1-3 berichten.`
    );
}

/**
 * Flatten AI generated message bursts into individual messages and add deelnemer IDs.
 * Each burst (1-3 messages from one politicus) becomes multiple individual GeneratedMessage objects.
 * @param bursts - The generated message bursts from the AI.
 * @param deelnemers - The deelnemers of the conversation.
 * @returns Flattened array of individual messages with deelnemer IDs.
 */
export function addDeelnemerIdsToAIGeneratedMessages(
  bursts: AIGeneratedMessage[],
  deelnemers: Deelnemer[]
): GeneratedMessage[] {
  const deelnemerNameToId = Object.fromEntries(
    deelnemers.map((d) => [d.name, d.id])
  );

  // Flatten bursts into individual messages
  const flattenedMessages: GeneratedMessage[] = [];

  for (const burst of bursts) {
    const deelnemerId = deelnemerNameToId[burst.deelnemerName];

    for (const message of burst.messages) {
      flattenedMessages.push({
        message,
        deelnemerName: burst.deelnemerName,
        deelnemerId,
      });
    }
  }

  return flattenedMessages;
}

/**
 * Build role and objective section
 */
function buildRoleSection(onderwerp: string): string {
  return `
    # Rol
    Je bent een expert in het schrijven van realistische Nederlandse politieke onderhandelingen. 
    Je schrijft een constructief gesprek tussen politici die tot een compromis proberen te komen.
    
    # Onderwerp
    ${onderwerp}`;
}

/**
 * Build deelnemers section with their details
 */
function buildDeelemersSection(sortedDeelnemers: Deelnemer[]): string {
  return `
    
    # Deelnemers (gesorteerd op zetels)
    ${sortedDeelnemers
      .map(
        (deelnemer) => `
    - ${deelnemer.name}
      - Partij: ${deelnemer.partij.name} (${deelnemer.partij.zetels} zetels)
      - Tone of voice: ${deelnemer.toneOfVoice}${
          deelnemer.persoonlijkeDetails
            ? `\n      - Achtergrond: ${deelnemer.persoonlijkeDetails}`
            : ""
        }${
          deelnemer.typischeUitspraken &&
          deelnemer.typischeUitspraken.length > 0
            ? `\n      - Typische uitspraken: ${deelnemer.typischeUitspraken.join(
                "; "
              )}`
            : ""
        }
      - Standpunten:
          ${deelnemer.partij.programma.standpunten
            .map((standpunt) => `- ${standpunt}`)
            .join("\n          ")}
    `
      )
      .join("\n")}`;
}

/**
 * Build zetelverdeling section
 */
function buildZetelverdeling(sortedDeelnemers: Deelnemer[]): string {
  const totalZetels = sortedDeelnemers.reduce(
    (sum, d) => sum + d.partij.zetels,
    0
  );
  const majorityNeeded = 76;

  return `
    
    # Zetelverdeling & Machtsbalans
    - Totaal zetels aan tafel: ${totalZetels}
    - Meerderheid nodig: ${majorityNeeded} zetels
    - Grootste partij: ${sortedDeelnemers[0].partij.name} (${
    sortedDeelnemers[0].name
  }, ${sortedDeelnemers[0].partij.zetels} zetels)
    - Kleinste partij: ${
      sortedDeelnemers[sortedDeelnemers.length - 1].partij.name
    } (${sortedDeelnemers[sortedDeelnemers.length - 1].name}, ${
    sortedDeelnemers[sortedDeelnemers.length - 1].partij.zetels
  } zetels)
    
    Politici kunnen zetels strategisch inzetten:
    - Grote partijen (>20 zetels): mandaat benadrukken
    - Kleine partijen: unieke positie als "kingmaker"
    - Coalitiekansen benoemen
    - Realisme tonen over macht`;
}

/**
 * Build chat-style format section
 */
function buildChatStyleSection(): string {
  return `
    
    # Chat-stijl Format
    - Dit is een GROEPSCHAT. Politici sturen korte berichten zoals in WhatsApp
    - Per beurt 1-3 berichten:
      * MEESTAL 1 bericht (korte reactie)
      * SOMS 2 berichten (uitgebreider)
      * ZELDEN 3 berichten (complex punt)
    - Berichten zijn ZEER KORT (meestal 1 korte zin, max 15-20 woorden)
    - Emojis: spaarzaam, vooral door jongere politici (Rob Jetten, Laurens Dassen)`;
}

/**
 * Build reactive flow section
 */
function buildReactiveFlowSection(): string {
  return `
    
    # Reactieve Flow
    - Politici reageren direct op wat er net gezegd is
    - Bepaal logisch wie getriggerd wordt:
      * Tegenpolen: Wilders ↔ Timmermans
      * Coalitiegenoten steunen elkaar
    - Reacties moeten logisch aansluiten op laatste berichten`;
}

/**
 * Build Kamer bevoegdheden section
 */
function buildKamerBevoegdhedenSection(): string {
  return `
    
    # Kamer Bevoegdheden
    
    ✅ WEL: Moties, wetsvoorstellen, begrotingswijzigingen, hoorzittingen
    ❌ NIET: Direct onderhandelen met bedrijven, uitvoerende taken
    
    Gebruik realistische instrumenten:
    - "Ik dien een motie in..."
    - "We kunnen via de begroting..."
    - "Laten we wetgeving aanpassen..."`;
}

/**
 * Build user prompt for the AI
 */
function buildUserPrompt(options: {
  conversationContext: string;
  lastSpeakerContext: string;
  speakerDistributionContext: string;
  phase: "begin" | "midden" | "einde";
  phaseInstructions: string;
  isLastBatch: boolean;
}): string {
  const baseTurnInstructions = options.isLastBatch
    ? "⚠️ DIT IS HET LAATSTE STUK - GENEREER 2 OF 3 BEURTEN ⚠️\n\nHet gesprek moet eindigen met toenadering en perspectief op een compromis."
    : "⚠️ GENEREER 2 OF 3 BEURTEN ⚠️";

  return `${options.conversationContext}${options.lastSpeakerContext}${options.speakerDistributionContext}

## Fase: ${options.phase.toUpperCase()}
${options.phaseInstructions}

${baseTurnInstructions}

Aantal beurten: 2 of 3
Berichten per beurt: MEESTAL 1, SOMS 2, ZELDEN 3
Maximum 3 politici mogen reageren.

WIE zou logisch reageren op de LAATSTE BERICHTEN? Overweeg:
- Wie wordt getriggerd door wat er in de LAATSTE BERICHTEN gezegd is?
- Reageer op de INHOUD van die specifieke berichten
${options.isLastBatch ? "- Laat het gesprek natuurlijk naar een conclusie vloeien" : ""}`;
}

/**
 * Build conversation summary context
 */
function buildConversationSummaryContext(
  messageCount: number,
  onderwerp: string
): string {
  if (messageCount > CONTEXT_THRESHOLD) {
    return `\n## Context:\nEr zijn al ${messageCount} berichten uitgewisseld. Het gesprek over "${onderwerp}" is in volle gang.\n\n⚠️ LET OP: Reageer ALLEEN op de LAATSTE BERICHTEN hieronder, niet op eerdere discussiepunten.\n`;
  } else if (messageCount > 0) {
    return `\n## Context:\nHet gesprek heeft ${messageCount} berichten. Reageer ALLEEN op de LAATSTE BERICHTEN hieronder.\n`;
  }
  return "";
}

/**
 * Build last speaker context (recent messages)
 */
function buildLastSpeakerContext(
  recentMessages: GeneratedMessage[]
): string {
  if (recentMessages.length === 0) return "";

  return `\n## ⚠️ LAATSTE BERICHTEN - DIT IS ALLES WAT JE HEBT:\n${recentMessages
    .map((m, i) => `[${i + 1}] ${m.deelnemerName}: "${m.message}"`)
    .join("\n")}\n\n🚨 KRITIEKE REGEL: Reageer UITSLUITEND op wat HIERBOVEN staat. NIETS anders.

Checklist voor ELKE reactie:
1. Als je iemand bij naam noemt: staat die persoon hierboven? ✓
2. Als je een argument bekritiseert: staat dat argument hierboven? ✓
3. Als je zetels noemt: is dat relevant voor wat hierboven gezegd is? ✓
`;
}

/**
 * Build speaker distribution context
 */
function buildSpeakerDistributionContext(
  deelnemers: Deelnemer[],
  speakerCounts: Map<string, number>
): string {
  const deelnemersBySpeechCount = deelnemers
    .map((d) => ({
      name: d.name,
      count: speakerCounts.get(d.name) || 0,
    }))
    .sort((a, b) => a.count - b.count);

  const leastActiveDeelnemers = deelnemersBySpeechCount
    .filter((d) => d.count < LEAST_ACTIVE_THRESHOLD)
    .map((d) => d.name);

  if (leastActiveDeelnemers.length > 0) {
    return `\n## BELANGRIJK - Sprekersverdeling:\nDeze politici hebben nog weinig gesproken: ${leastActiveDeelnemers.join(", ")}\nVarieer de sprekers!\n`;
  }
  return "\n## BELANGRIJK - Sprekersverdeling:\nVarieer de sprekers! Laat ook andere politici aan het woord.\n";
}

/**
 * Build the system message for the AI.
 * @param opties - The options for the conversation.
 * @returns The system message for the AI.
 */
const buildSystemMessage = (opties: {
  onderwerp: string;
  deelnemers: Deelnemer[];
}) => {
  const sortedDeelnemers = [...opties.deelnemers].sort(
    (a, b) => b.partij.zetels - a.partij.zetels
  );

  return (
    buildRoleSection(opties.onderwerp) +
    buildDeelemersSection(sortedDeelnemers) +
    buildZetelverdeling(sortedDeelnemers) +
    buildChatStyleSection() +
    buildReactiveFlowSection() +
    buildKamerBevoegdhedenSection()
  );
};

export async function genereerGesprekBerichten(opties: {
  onderwerp: string;
  deelnemers: Deelnemer[];
  startAt: Date;
  endAt: Date;
}) {
  const generatedMessages: GeneratedMessage[] = [];
  const systemPrompt = buildSystemMessage(opties);

  let phase: "begin" | "midden" | "einde" = "begin";
  let batchCount = 0;

  logger.info(
    `Generating ~${GESPREK_TARGET_MESSAGES_COUNT} messages in batches of ${GESPREK_TURNS_PER_BATCH} turns (kleinere batches voor natuurlijker gesprek)`,
    { phase }
  );

  while (generatedMessages.length < GESPREK_TARGET_MESSAGES_COUNT) {
    batchCount++;

    const progress = generatedMessages.length / GESPREK_TARGET_MESSAGES_COUNT;

    if (progress < 0.3) {
      phase = "begin";
    } else if (progress < 0.6) {
      phase = "midden";
    } else {
      phase = "einde";
    }

    const remainingMessages =
      GESPREK_TARGET_MESSAGES_COUNT - generatedMessages.length;

    const isLastBatch = remainingMessages <= LAST_BATCH_THRESHOLD;

    logger.debug(
      `Generating batch ${batchCount} with ${GESPREK_TURNS_PER_BATCH} turns (${remainingMessages} messages remaining)`,
      { phase, remainingMessages }
    );

    // Build context for AI prompt
    const conversationSummaryContext = buildConversationSummaryContext(
      generatedMessages.length,
      opties.onderwerp
    );

    const phaseInstructions = {
      begin:
        "Dit is het BEGIN van het gesprek. Laat de politici het onderwerp introduceren en hun standpunten helder uiteenzetten. Toon waar ze van elkaar verschillen.",
      midden:
        "Dit is het MIDDEN van het gesprek. De discussie wordt dieper. Politici stellen kritische vragen, uiten bezwaren, maar beginnen ook overeenkomsten te zien. Toon realistische spanning maar ook openheid.",
      einde:
        "Dit is het EINDE van het gesprek. Politici moeten nu naar elkaar toebewegen. Laat zien dat er ruimte is voor een compromis of dat er concrete vervolgstappen worden gezet. Eindig constructief met perspectief op een oplossing.",
    };

    const recentMessages = generatedMessages.slice(-RECENT_MESSAGES_COUNT);
    const lastSpeakerContext = buildLastSpeakerContext(recentMessages);

    // Track speaker counts
    const speakerCounts = new Map<string, number>();
    generatedMessages.forEach((m) => {
      speakerCounts.set(
        m.deelnemerName,
        (speakerCounts.get(m.deelnemerName) || 0) + 1
      );
    });

    const speakerDistributionContext = buildSpeakerDistributionContext(
      opties.deelnemers,
      speakerCounts
    );

    const userPrompt = buildUserPrompt({
      conversationContext: conversationSummaryContext,
      lastSpeakerContext,
      speakerDistributionContext,
      phase,
      phaseInstructions: phaseInstructions[phase],
      isLastBatch,
    });

    try {
      // Selecteer random model configuratie voor variatie
      const modelConfig = getRandomModelConfig();

      logger.debug(`Calling AI with schema`, {
        schemaStructure:
          "Array of turns, each turn has deelnemerName and messages array",
        modelConfig: modelConfig.name,
        temperature: modelConfig.settings.temperature,
      });

      const response = await generateObject({
        model: modelConfig.model,
        ...modelConfig.settings,
        schema: buildMessageSchema(opties.deelnemers),
        schemaName: "PolitiekeGespreksBeurten",
        schemaDescription:
          "Array van EXACT 2 of 3 politieke gespreksbeurten. Elke beurt bevat de naam van een politicus en hun 1-3 korte berichten. ABSOLUUT MAXIMUM: 3 beurten. Niet 4, niet 5, maximaal 3.",
        mode: "json",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt },
        ],
        // @ts-expect-error - onFinish is not a valid property
        onFinish: ({ response: finishResponse }) => {
          logger.debug(`AI raw response (onFinish)`, {
            text: finishResponse.text,
            finishReason: finishResponse.finishReason,
            modelUsed: modelConfig.name,
          });
        },
      });

      logger.debug(`AI response received successfully`, {
        turnsGenerated: response.object.length,
        rawResponse: JSON.stringify(response.object, null, 2),
      });

      // Truncate naar max turns - AI genereert vaak meer turns ondanks instructies
      const aiMessages =
        response.object.length > MAX_TURNS_TO_USE
          ? response.object.slice(0, MAX_TURNS_TO_USE)
          : response.object;

      if (response.object.length > MAX_TURNS_TO_USE) {
        logger.info(
          `AI generated ${response.object.length} turns, using first ${MAX_TURNS_TO_USE}`,
          {
            generated: response.object.length,
            used: aiMessages.length,
          }
        );
      }

      const newMessages = addDeelnemerIdsToAIGeneratedMessages(
        aiMessages,
        opties.deelnemers
      );

      if (newMessages.length === 0) {
        logger.warn(`No messages generated for batch ${batchCount}`, {
          batchCount,
        });
        break;
      }

      generatedMessages.push(...newMessages);
      logger.debug(
        `Generated ${newMessages.length} messages for batch ${batchCount} (total: ${generatedMessages.length}/${GESPREK_TARGET_MESSAGES_COUNT})`,
        { batchCount, totalMessages: generatedMessages.length }
      );

      if (generatedMessages.length >= GESPREK_TARGET_MESSAGES_COUNT) {
        logger.info(`Target message count reached, stopping generation`);
        break;
      }
    } catch (error) {
      // Log detailed error information including any raw response
      const errorDetails: {
        errorName: string;
        errorMessage: string;
        rawResponse?: unknown;
        rawText?: unknown;
        cause?: unknown;
      } = {
        errorName: error instanceof Error ? error.name : "Unknown",
        errorMessage: error instanceof Error ? error.message : String(error),
      };

      // Try to extract raw response from AI SDK error
      if (error && typeof error === "object") {
        // @ts-expect-error - AI SDK errors may have these properties
        if (error.response) errorDetails.rawResponse = error.response;
        // @ts-expect-error - AI SDK errors may have these properties
        if (error.text) errorDetails.rawText = error.text;
        // @ts-expect-error - AI SDK errors may have these properties
        if (error.cause) errorDetails.cause = error.cause;
      }

      logger.error(
        `Error generating messages for batch ${batchCount}`,
        errorDetails
      );

      if (generatedMessages.length === 0) {
        throw error;
      }
      break;
    }
  }

  logger.info("Gesprek berichten generation completed", {
    totalMessages: generatedMessages.length,
    targetMessages: GESPREK_TARGET_MESSAGES_COUNT,
  });

  return addTimestampsToMessages(
    generatedMessages,
    opties.startAt,
    opties.endAt
  );
}
